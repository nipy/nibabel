#!/usr/bin/env python
"""PAR/REC to NIfTI converter
"""
from __future__ import division, print_function, absolute_import

from optparse import OptionParser, Option
import numpy as np
import sys
import os
import gzip
import nibabel
import nibabel.parrec as pr
import nibabel.nifti1 as nifti1
from nibabel.filename_parser import splitext_addext
from nibabel.volumeutils import seek_tell

# global verbosity switch
verbose_switch = False


def get_opt_parser():
    # use module docstring for help output
    p = OptionParser(
        usage="%s [OPTIONS] <PAR files>\n\n" % sys.argv[0] + __doc__,
        version="%prog " + nibabel.__version__)

    p.add_option(
        Option("-v", "--verbose", action="store_true", dest="verbose",
               default=False, help="Make some noise."))
    p.add_option(
        Option("-o", "--output-dir", action="store", type="string",
               dest="outdir", default=None, help="""
Destination directory for NIfTI files. Default: current directory."""))
    p.add_option(
        Option("-c", "--compressed", action="store_true",
               dest="compressed", default=False,
               help="Whether to write compressed NIfTI files or not."))
    p.add_option(
        Option("-p", "--permit-truncated", action="store_true",
               dest="permit_truncated", default=False, help="""
Permit conversion of truncated recordings. Support for this is experimental,
and results *must* be checked afterward for validity."""))
    p.add_option(
        Option("-b", "--bvs", action="store_true", dest="bvs", default=False,
               help="""
Output bvals/bvecs files in addition to NIFTI image."""))
    p.add_option(
        Option("-d", "--dwell-time", action="store_true", default=False,
               dest="dwell_time",
               help="""
Calculate the scan dwell time. If supplied, the magnetic field strength
should also be supplied using --field-strength (default 3). The field strength
must be supplied because it is not encoded in the PAR/REC format."""))
    p.add_option(
        Option("--field-strength", action="store", default=3., type="float",
               dest="field_strength", help="""
The magnetic field strength of the recording, only needed for --dwell-time.
The field strength must be supplied because it is not encoded in the PAR/REC
format."""))
    p.add_option(
        Option("--origin", action="store", dest="origin", default="scanner",
               help="""
Reference point of the q-form transformation of the NIfTI image. If 'scanner'
the (0,0,0) coordinates will refer to the scanner's iso center. If 'fov', this
coordinate will be the center of the recorded volume (field of view). Default:
'scanner'."""))
    p.add_option(
        Option("--minmax", action="store", nargs=2, dest="minmax", help="""
Mininum and maximum settings to be stored in the NIfTI header. If any of
them is set to 'parse', the scaled data is scanned for the actual minimum and
maximum. To bypass this potentially slow and memory intensive step (the data
has to be scaled and fully loaded into memory), fixed values can be provided as
space-separated pair, e.g. '5.4 120.4'. It is possible to set a fixed minimum
as scan for the actual maximum (and vice versa). Default: 'parse parse'."""))
    p.set_defaults(minmax=('parse', 'parse'))
    p.add_option(
        Option("--store-header", action="store_true", dest="store_header",
               default=False, help="""
If set, all information from the PAR header is stored in an extension of
the NIfTI file header. Default: off"""))
    p.add_option(
        Option("--scaling", action="store", dest="scaling", default='dv',
               help="""
Choose data scaling setting. The PAR header defines two different data
scaling settings: 'dv' (values displayed on console) and 'fp' (floating point
values). Either one can be chosen, or scaling can be disabled completely
('off').  Note that neither method will actually scale the data, but just store
the corresponding settings in the NIfTI header, unless non-uniform scaling
is used, in which case the data is stored in the file in scaled form.
Default: 'dv'"""))
    p.add_option(
        Option("--overwrite", action="store_true", dest="overwrite",
               default=False, help="""
Overwrite file if it exists. Default: False"""))
    return p


def verbose(msg, indent=0):
    if verbose_switch:
        print("%s%s" % (' ' * indent, msg))


def error(msg, exit_code):
    sys.stderr.write(msg + '\n')
    sys.exit(exit_code)


def proc_file(infile, opts):
    # figure out the output filename, and see if it exists
    basefilename = splitext_addext(os.path.basename(infile))[0]
    if opts.outdir is not None:
        # set output path
        basefilename = os.path.join(opts.outdir, basefilename)

    # prep a file
    if opts.compressed:
        verbose('Using gzip compression')
        outfilename = basefilename + '.nii.gz'
    else:
        outfilename = basefilename + '.nii'
    if os.path.isfile(outfilename) and not opts.overwrite:
        raise IOError('Output file "%s" exists, use --overwrite to '
                      'overwrite it' % outfilename)

    # load the PAR header
    pr_img = pr.load(infile, opts.permit_truncated)
    pr_hdr = pr_img.header

    # get the raw unscaled data form the REC file
    raw_data = pr_img.dataobj.get_unscaled()

    # determine if our order is XYTZ instead of XYZT (Phillips scanners)
    # If it's XYZT, then the slice number (Z) will iterate slowest (no diff)
    order_rev = pr_hdr.order_xytz
    if raw_data.ndim > 3 and order_rev:
        verbose('XYTZ order detected, reordering data')
        assert raw_data.flags['C_CONTIGUOUS'] is True
        reorder = [0, 1, 3, 2]
        raw_data = np.reshape(raw_data, [raw_data.shape[n] for n in reorder],
                              order='F')
        raw_data = np.transpose(raw_data, reorder)

    # compute affine with desired origin
    affine = pr_hdr.get_affine(origin=opts.origin)

    # create an nifti image instance -- to get a matching header
    nimg = nifti1.Nifti1Image(raw_data, affine)
    nhdr = nimg.header

    if 'parse' in opts.minmax:
        # need to get the scaled data
        verbose('Loading (and scaling) the data to determine value range')
        if opts.scaling == 'off':
            scaled_data = raw_data
        else:
            slope, intercept = pr_hdr.get_data_scaling(method=opts.scaling)
            scaled_data = slope * raw_data + intercept
    if opts.minmax[0] == 'parse':
        nhdr.structarr['cal_min'] = scaled_data.min()
    else:
        nhdr.structarr['cal_min'] = float(opts.minmax[0])
    if opts.minmax[1] == 'parse':
        nhdr.structarr['cal_max'] = scaled_data.max()
    else:
        nhdr.structarr['cal_max'] = float(opts.minmax[1])

    # container for potential NIfTI1 header extensions
    exts = nifti1.Nifti1Extensions()

    if opts.store_header:
        # dump the full PAR header content into an extension
        fobj = open(infile, 'r')
        hdr_dump = fobj.read()
        dump_ext = nifti1.Nifti1Extension('comment', hdr_dump)
        fobj.close()
        exts.append(dump_ext)

    # put any extensions into the image
    nimg.extra['extensions'] = exts

    # image description
    descr = "%s;%s;%s;%s" % (
        pr_hdr.general_info['exam_name'],
        pr_hdr.general_info['patient_name'],
        pr_hdr.general_info['exam_date'].replace(' ', ''),
        pr_hdr.general_info['protocol_name'])
    nhdr.structarr['descrip'] = descr[:80]

    if pr_hdr.general_info['max_dynamics'] > 1:
        # fMRI
        nhdr.structarr['pixdim'][4] = pr_hdr.general_info['repetition_time']
        # store units -- always mm and msec
        nhdr.set_xyzt_units('mm', 'msec')
    else:
        # anatomical or DTI
        nhdr.set_xyzt_units('mm', 'unknown')

    # get original scaling, and decide if we scale in-place or not
    scale_data = False
    if opts.scaling == 'off':
        slope = 1.
        intercept = 0.
    else:
        verbose('Using data scaling "%s"' % opts.scaling)
        slope, intercept = pr_hdr.get_data_scaling(method=opts.scaling)
        uni_s = np.unique(slope)
        uni_i = np.unique(intercept)
        if len(uni_s) == 1 and len(uni_i) == 1:
            slope = uni_s[0]
            intercept = uni_i[0]
        else:
            verbose("Multiple scale-factors detected, data will be saved "
                    "in scaled form")
            scale_data = True

    # finalize the header: set proper data offset, pixdims, ...
    nimg.update_header()

    # prep a file
    if opts.compressed:
        verbose('Using gzip compression')
        outfile = gzip.open(outfilename, 'wb')
    else:
        outfile = open(outfilename, 'wb')

    try:
        verbose('Writing %s' % outfilename)
        # first write the header
        if scale_data:
            data = ((raw_data * slope) + intercept).astype(raw_data.dtype)
            nhdr.set_slope_inter(1., 0.)
        else:
            nhdr.set_slope_inter(slope, intercept)
            data = raw_data
        nhdr.write_to(outfile)
        # Now the data
        offset = nhdr.get_data_offset()
        seek_tell(outfile, offset, write0=True)
        nibabel.volumeutils.array_to_file(data, outfile, offset=offset)
    finally:
        outfile.close()

    # write out bvals/bvecs if requested
    if opts.bvs and pr_hdr.general_info['n_dti_volumes'] > 1:
        verbose('Writing .bvals and .bvecs files')
        # bvals
        gen_info = pr_hdr.general_info
        shape = (gen_info['n_dti_volumes'], gen_info['n_slices'])
        shape = (shape[1], shape[0]) if order_rev else shape
        bvals = np.reshape(pr_hdr.image_defs['diffusion_b_factor'], shape)
        bvals = bvals.T if order_rev else bvals
        if not np.all(np.diff(bvals, axis=1) == 0):
            raise RuntimeError('Could not output bvals: inconsistent values')

        # bvecs
        shape = (3, pr_hdr.general_info['n_dti_volumes'],
                 pr_hdr.general_info['n_slices'])
        shape = (shape[0], shape[2], shape[1]) if order_rev else shape
        bvecs = np.reshape(pr_hdr.image_defs['diffusion'].T, shape)
        bvecs = np.swapaxis(bvecs, 2, 1) if order_rev else bvecs
        if not np.all(np.diff(bvecs, axis=2) == 0):
            raise RuntimeError('Could not output bvecs: inconsistent values')
        bvecs = bvecs[:, :, 0]

        # restore XYZ order to bvecs
        assert np.all(bvecs[:, 0] == 0)  # first col should be zeros
        order = []
        mults = []
        for ii in range(3):
            idx = np.where(bvecs[:, ii+1] != 0)[0]
            assert len(idx) == 1
            order.append(idx[0])
            mults.append(bvecs[idx[0], ii+1])
        assert np.array_equal(np.unique(order), [0, 1, 2])
        assert np.array_equal(np.abs(mults), [1, 1, 1])
        bvecs = bvecs[order] * np.array(mults)[:, np.newaxis]
        bvecs[bvecs == -0.0] = 0.0

        # Actually write files
        bval_fname = basefilename + '.bvals'
        bvec_fname = basefilename + '.bvecs'
        with open(bval_fname, 'w') as fid:
            # np.savetxt could do this, but it's just a loop anyway
            for val in bvals[:, 0]:
                fid.write('%s ' % val)
            fid.write('\n')
        with open(bvec_fname, 'w') as fid:
            for row in bvecs:
                for val in row:
                    fid.write('%s ' % val)
                fid.write('\n')
    elif opts.bvs:
        verbose('No DTI volumes detected, bvals and bvecs not written')
    if opts.dwell_time:
        dwell_time = pr_hdr.get_dwell_time(opts.field_strength)
        if dwell_time is None:
            verbose('No EPI factors, dwell time not written')
        else:
            verbose('Writing dwell time (%r sec) calculated assuming %sT '
                    'magnet' % (dwell_time, opts.field_strength))
            dwell_fname = basefilename + '.dwell_time'
            with open(dwell_fname, 'w') as fid:
                fid.write('%r\n' % dwell_time)
    # done


def main():
    parser = get_opt_parser()
    (opts, infiles) = parser.parse_args()

    global verbose_switch
    verbose_switch = opts.verbose

    if opts.origin not in ['scanner', 'fov']:
        error("Unrecognized value for --origin: '%s'." % opts.origin, 1)

    # store any exceptions
    errs = []
    for infile in infiles:
        verbose('Processing %s' % infile)
        try:
            proc_file(infile, opts)
        except Exception as e:
            errs.append('%s: %s' % (infile, e))

    if len(errs):
        error('Caught %i exceptions. Dump follows:\n\n %s'
              % (len(errs), '\n'.join(errs)), 1)
    else:
        verbose('Done')


if __name__ == '__main__':
    main()
